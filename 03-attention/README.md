# 03 - Attention (Auto-AtenÃ§Ã£o)

ImplementaÃ§Ã£o de modelo de linguagem neural com **mecanismo de auto-atenÃ§Ã£o (Self-Attention)**, explorando os fundamentos do paper "Attention is All You Need" (Vaswani et al., 2017).


## ğŸ¯ Objetivos

- Implementar camada de auto-atenÃ§Ã£o (self-attention)
- Desenvolver duas versÃµes: **com loops** (didÃ¡tica) e **matricial** (eficiente)
- Integrar embeddings de posiÃ§Ã£o
- Implementar projeÃ§Ãµes lineares (WQ, WK, WV, WO)
- Adicionar camada feed-forward (MLP de 2 camadas)
- Treinar modelo de linguagem com atenÃ§Ã£o
- Comparar com modelo sem atenÃ§Ã£o (Bengio 2003)

## ğŸ—‚ï¸ Estrutura

```
03-attetion/
â”œâ”€â”€ README.md
â”œâ”€â”€ auto_atenÃ§Ã£o.ipynb
â””â”€â”€ comentario-resumo-attetion-is-all-you-need.pdf
```

## ğŸ“š FundamentaÃ§Ã£o TeÃ³rica

### Mecanismo de Auto-AtenÃ§Ã£o

O mecanismo de auto-atenÃ§Ã£o permite que o modelo "preste atenÃ§Ã£o" a diferentes partes da sequÃªncia de entrada ao processar cada token.

**Componentes principais:**

1. **Query (Q)**: "O que estou procurando?"
2. **Key (K)**: "O que eu tenho para oferecer?"
3. **Value (V)**: "O que eu realmente represento?"

**FÃ³rmula:**
```
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V
```

### Arquitetura

```
Input (context_size tokens)
    â†“
Token Embedding + Positional Embedding
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Self-Attention Layer               â”‚
â”‚  â”œâ”€ Linear Projections (WQ,WK,WV)  â”‚
â”‚  â”œâ”€ Scaled Dot-Product Attention   â”‚
â”‚  â””â”€ Output Projection (WO)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Feed-Forward Network (2-layer MLP)
    â†“
Output Layer (vocab_size)
    â†“
Softmax (distribuiÃ§Ã£o de probabilidade)
```


## ğŸ“Š ComparaÃ§Ã£o de Performance

| Aspecto | Com Loops | Matricial |
|---------|-----------|-----------|
| **Tempo/batch** | ~500ms | ~50ms |
| **Velocidade** | Baseline | **10x mais rÃ¡pido** |
| **Uso de GPU** | Baixo | Alto |
| **ParalelizaÃ§Ã£o** | NÃ£o | Sim |
| **DidÃ¡tica** | âœ… Excelente | âš ï¸ Complexa |


## ğŸ” AnÃ¡lise e Insights

### Vantagens da Auto-AtenÃ§Ã£o

âœ… **Captura dependÃªncias longas**: Tokens distantes podem se "ver"  
âœ… **ParalelizaÃ§Ã£o**: OperaÃ§Ãµes matriciais eficientes em GPU  
âœ… **Flexibilidade**: Funciona com sequÃªncias de tamanho variÃ¡vel  
âœ… **Interpretabilidade**: Pesos de atenÃ§Ã£o sÃ£o visualizÃ¡veis


### Aprendizados

1. **Loops vs Matricial**: OperaÃ§Ãµes vetorizadas sÃ£o muito mais rÃ¡pidas
2. **AtenÃ§Ã£o Ã© Contextual**: Cada token considera todos os outros
3. **Embeddings Posicionais**: Cruciais para ordem da sequÃªncia
4. **ProjeÃ§Ãµes Lineares**: WQ, WK, WV aprendem representaÃ§Ãµes Ãºteis
5. **Escalabilidade**: Base para Transformers modernos (BERT, GPT)


## ğŸ“– ReferÃªncia Original

**Vaswani, A., et al. (2017)**  
[*"Attention is All You Need"*](https://arxiv.org/abs/1706.03762)  
Advances in Neural Information Processing Systems (NIPS)

**ContribuiÃ§Ãµes do Paper:**
- IntroduÃ§Ã£o do Transformer (arquitetura puramente baseada em atenÃ§Ã£o)
- Multi-head attention
- Positional encodings
- Estado da arte em traduÃ§Ã£o automÃ¡tica

## ğŸ“ Notas de ImplementaÃ§Ã£o

- **Dataset**: Obras de Machado de Assis (mesmo do exercÃ­cio anterior)
- **Duas versÃµes**: Loop (didÃ¡tica) + Matricial (produÃ§Ã£o)
- **ValidaÃ§Ã£o**: Assert garante equivalÃªncia entre implementaÃ§Ãµes
- **Treinamento**: Apenas com versÃ£o matricial (eficiÃªncia)
- **ComparaÃ§Ã£o**: Modelo com/sem atenÃ§Ã£o
- **Material de apoio**: PDF com resumo do paper original

## ğŸ”— Arquivos do Projeto

- `auto_atenÃ§Ã£o.ipynb` - ImplementaÃ§Ã£o completa
- `comentario-resumo-attetion-is-all-you-need.pdf` - Resumo do paper