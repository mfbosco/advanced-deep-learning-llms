# 02 - Language Model (Modelo de Linguagem - Bengio 2003)

Implementa√ß√£o de um modelo de linguagem neural baseado no trabalho seminal de **Bengio et al. (2003)**, utilizando embeddings de palavras e redes neurais MLP para prever a pr√≥xima palavra em uma sequ√™ncia.

## üìã Descri√ß√£o

Este projeto implementa um modelo de linguagem estat√≠stico neural que aprende a prever a pr√≥xima palavra dado um contexto de palavras anteriores. A abordagem utiliza embeddings aprend√≠veis e uma arquitetura feedforward simples, representando um dos primeiros usos bem-sucedidos de redes neurais para modelagem de linguagem.

## üéØ Objetivos

- Implementar modelo de linguagem neural do tipo feedforward
- Utilizar embeddings de palavras aprend√≠veis
- Treinar modelo para previs√£o da pr√≥xima palavra
- Calcular perplexidade (m√©trica de avalia√ß√£o)
- Gerar texto de forma autoregressiva
- Alcan√ßar perplexidade < 200


##  Fundamenta√ß√£o Te√≥rica

### Modelo de Linguagem Neural (Bengio 2003)

O modelo proposto por Bengio revolucionou a √°rea de NLP ao introduzir:

1. **Word Embeddings**: Representa√ß√µes vetoriais densas e de baixa dimensionalidade
2. **Arquitetura Neural**: MLP para capturar depend√™ncias entre palavras
3. **Aprendizado Conjunto**: Embeddings e pesos da rede aprendidos simultaneamente

### Arquitetura

```
Input (context_size palavras) 
    ‚Üì
Embedding Layer (vocab_size ‚Üí embedding_dim)
    ‚Üì
Concatena√ß√£o dos embeddings
    ‚Üì
Hidden Layer (n√£o-linear)
    ‚Üì
Output Layer (‚Üí vocab_size)
    ‚Üì
Softmax (distribui√ß√£o de probabilidade)
```

## üõ†Ô∏è Implementa√ß√£o

### 1. Prepara√ß√£o de Dados

**Dataset**: Obras de Machado de Assis (pr√©-processado)  
**Vocabul√°rio**: 2001 tokens (top 2000 + `<unk>`)  
**Context Size**: 5 palavras anteriores  
**Target**: Pr√≥xima palavra (6¬™ palavra)

### 2. Dataset PyTorch

Implementa√ß√£o da classe `MachadoDataset`:

```python
class MachadoDataset(Dataset):
    """Dataset para modelagem de linguagem com contexto.
    
    Attributes:
        X: contextos (N, context_size)
        Y: targets (N,)
        context_size: Tamanho do contexto (janela de tokens)
    """
    
    def __init__(self, X, Y, context_size=5):
        # Valida√ß√µes
        assert len(X) == len(Y), "N√∫mero de contextos e alvos deve ser igual"
        assert all(len(ctx) == context_size for ctx in X)
        
        # Converte para tensores
        self.X = torch.tensor([[x for x in ctx] for ctx in X], dtype=torch.long)
        self.Y = torch.tensor([y for y in Y], dtype=torch.long)
        self.context_size = context_size
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.Y[idx]
```

**Caracter√≠sticas:**
- Split: 80% treino / 20% valida√ß√£o
- Remo√ß√£o de tokens `<unk>` (√≠ndice 0)
- Janela deslizante para criar exemplos
- Formato: (context_tensor, target_tensor)
- Convers√£o imediata para tensores PyTorch

### 3. Arquitetura do Modelo

```python
class LanguageModel(nn.Module):
    def __init__(self, vocab_size=2001, embedding_dim=128, 
                 hidden_dim=512, context_size=5):
        super(LanguageModel, self).__init__()
        
        # Camada de embedding
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        # Camada oculta (n√£o-linear)
        self.hidden = nn.Sequential(
            nn.Linear(context_size * embedding_dim, hidden_dim),
            nn.Tanh()
        )
        
        # Camada de sa√≠da
        self.output = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x):
        # x: (batch_size, context_size)
        embeds = self.embeddings(x)  # (batch, context, embed_dim)
        embeds = embeds.view(embeds.shape[0], -1)  # flatten
        hidden = self.hidden(embeds)  # (batch, hidden_dim)
        out = self.output(hidden)  # (batch, vocab_size)
        return out
```

**Hiperpar√¢metros (configura√ß√£o real):**
- `vocab_size`: 2001 (top 2000 + `<unk>`)
- `embedding_dim`: 128
- `hidden_dim`: 512
- `context_size`: 5
- `batch_size`: 256
- Ativa√ß√£o: **Tanh** (como no paper original)

### 4. Treinamento

**Configura√ß√£o:**
```python
epochs = 5
lr = 0.001
batch_size = 256
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
```

**Fun√ß√µes de Treino e Valida√ß√£o:**
```python
def train_batch(model, X, Y, optimizer, criterion, device):
    model.train()
    X, Y = X.to(device), Y.to(device)
    output = model(X)
    loss = criterion(output, Y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()

@torch.no_grad()
def validate_batch(model, X, Y, criterion, device):
    model.eval()
    X, Y = X.to(device), Y.to(device)
    output = model(X)
    loss = criterion(output, Y)
    return loss.item()
```

**Loop de Treino:**
- Valida√ß√£o de √≠ndices antes do treino (seguran√ßa)
- C√°lculo de loss e perplexidade por √©poca
- Monitoramento de tempo por √©poca
- Plots de loss e perplexidade ao longo das √©pocas

### 5. Avalia√ß√£o: Perplexidade

$perplexity = \exp(average_loss)$

**Interpreta√ß√£o:**
- Menor perplexidade = melhor modelo
- Perplexidade de 100 = modelo considera ~100 palavras igualmente prov√°veis
- **Meta**: Perplexidade < 200

### 6. Gera√ß√£o de Texto

Gera√ß√£o autoregressiva usando amostragem.

**Exemplos (portugu√™s - Machado de Assis):**
```python
generate_text(model, vocab, "Era uma dia belo de sol", max_length=9)
# Output: "Era uma dia belo de sol e a casa de"
```

## üîç Conceitos Abordados

- **Language Modeling**: Modelagem estat√≠stica de sequ√™ncias
- **Word Embeddings**: Representa√ß√µes vetoriais densas
- **N-gram Context**: Uso de contexto de tamanho fixo
- **Feedforward Neural Networks**: MLPs para NLP
- **Perplexity**: M√©trica de avalia√ß√£o de modelos de linguagem
- **Autoregressive Generation**: Gera√ß√£o sequencial de texto
- **Cross-Entropy Loss**: Fun√ß√£o objetivo para classifica√ß√£o

## üéì Aprendizados

1. **Embeddings vs One-Hot**: Embeddings capturam sem√¢ntica e reduzem dimensionalidade
2. **Context Window**: Trade-off entre contexto e complexidade
3. **Perplexity**: M√©trica intuitiva para modelos probabil√≠sticos
4. **Gera√ß√£o Autoregressiva**: Base para modelos modernos (GPT)
5. **Vocabul√°rio**: Tratamento de OOV √© crucial

## ÔøΩ Componentes e M√©tricas

| Componente | Descri√ß√£o | Valor/Tipo |
|------------|-----------|------------|
| **Dataset** | Obras de Machado de Assis | ~176K pares (X,Y) |
| **Vocabul√°rio** | Top tokens mais frequentes | 2001 |
| **Embedding Layer** | Converte tokens em vetores densos | dim=128 |
| **Hidden Layer** | Aprende representa√ß√µes n√£o-lineares | dim=512, Tanh |
| **Output Layer** | Gera distribui√ß√£o sobre vocabul√°rio | dim=2001 |
| **Batch Size** | Tamanho do lote de treinamento | 256 |
| **Perplexity** | M√©trica de avalia√ß√£o | < 200 (meta) |


## üìñ Refer√™ncia Original

**Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003)**  
[*"A Neural Probabilistic Language Model"*](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)  
Journal of Machine Learning Research, 3, 1137-1155

**Inova√ß√µes do Paper:**
- Primeira aplica√ß√£o bem-sucedida de embeddings
- Demonstra√ß√£o de que redes neurais superam n-gramas
- Base para Word2Vec, GloVe e modelos modernos


## üìù Notas de Implementa√ß√£o

- **Dataset**: Obras completas de Machado de Assis (dom√≠nio p√∫blico)
- **Pr√©-processamento**: Realizado no notebook `Prepara√ß√£o_de_dados.ipynb`
- **Cr√©ditos**: Dataset preparado por Augusto Zolet
- **Exerc√≠cio**: Desenvolvido com suporte de ChatGPT/Copilot
- **Foco**: Compreens√£o dos fundamentos de modelos de linguagem
- **Gera√ß√£o**: Textos em portugu√™s com estilo liter√°rio

---

**Material Educacional**: Implementa√ß√£o pr√°tica do modelo de Bengio 2003 aplicado a textos liter√°rios em portugu√™s.

