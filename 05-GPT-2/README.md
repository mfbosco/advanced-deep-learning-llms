# 05 - GPT-2 (MÃ¡scaras Causais e GeraÃ§Ã£o Autoregressiva)

ImplementaÃ§Ã£o de modelo de linguagem **autoregressivo** com **mÃ¡scara causal**, explorando a arquitetura GPT (Generative Pre-trained Transformer) para geraÃ§Ã£o de texto.

## ğŸ¯ Objetivo

Treinar modelo de linguagem que:
- Usa **mÃ¡scara causal** (impede acesso a tokens futuros)
- Gera texto de forma **autoregressiva** (token por token)
- Implementa **multi-head attention**
- Suporta tokens especiais `<sos>` e `<eos>`

## ğŸ—‚ï¸ Estrutura

```
05-GPT-2/
â”œâ”€â”€ README.md
â”œâ”€â”€ mascara_causal_gpt_2.ipynb
â””â”€â”€ comentario-GPT-2.pdf
```

## ğŸ“š Conceitos

**GPT** (Radford et al., 2018):
- **Decoder-only** architecture
- **Causal masking**: Token i nÃ£o vÃª tokens > i
- **Autoregressive**: Prediz prÃ³ximo token dado histÃ³rico
- GeraÃ§Ã£o de texto de alta qualidade


## ğŸ” GPT vs Modelos Anteriores

| Modelo | Contexto | GeraÃ§Ã£o | Performance |
|--------|----------|---------|-------------|
| Bengio 2003 | Fixo | Simples | Baseline |
| Attention | VariÃ¡vel | BÃ¡sica | Melhor |
| BERT | Bidirecional | âŒ NÃ£o gera | CompreensÃ£o |
| **GPT** | **Causal** | **âœ… Excelente** | **Estado da arte** |

## ğŸ“– ReferÃªncia

**Radford, A., et al. (2019)**  
[*"Language Models are Unsupervised Multitask Learners"*](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
OpenAI Technical Report


## ğŸ“ Aprendizados

1. **MÃ¡scara Causal**: Essencial para geraÃ§Ã£o autoregressiva
2. **Tokens Especiais**: `<sos>`, `<eos>` delimitam sequÃªncias
3. **Temperature**: Controla criatividade vs coerÃªncia
4. **Autoregressive**: Gera um token por vez condicionado no histÃ³rico
5. **GPT = BERT invertido**: Decoder vs Encoder
