# CLIP - Embedding Multimodal

## Objetivo

Implementar busca multimodal alinhando embeddings de **imagem** e **texto** em um espaÃ§o comum, permitindo recuperar imagens atravÃ©s de queries textuais (e vice-versa).

## Estrutura

```
07-CLIP/
â”œâ”€â”€ embedding_multimodal_clip.ipynb   # ImplementaÃ§Ã£o CLIP-like
â””â”€â”€ comentario_CLIP.pdf               # Material complementar
```

## Conceitos Principais

### Embeddings Multimodais
- **Objetivo**: Alinhar representaÃ§Ãµes de imagem e texto para que conceitos similares tenham embeddings prÃ³ximos
- **Exemplo**: Embedding da palavra "car" deve ser similar ao embedding de uma imagem de carro
- **AplicaÃ§Ã£o**: Busca de imagens por query textual usando similaridade de cosseno

### Modelos PrÃ©-treinados (Congelados)
- **Imagem**: EfficientNet-B0 (classificaÃ§Ã£o de imagens ImageNet)
- **Texto**: BERT base uncased (processamento de linguagem natural)
- **ProjeÃ§Ãµes**: Camadas lineares treinÃ¡veis que mapeiam embeddings para espaÃ§o comum

### FunÃ§Ãµes de Perda

#### 1. MSE Loss (Baseline)
- **Objetivo**: Minimizar distÃ¢ncia euclidiana entre pares positivos (imagem-texto correspondentes)
- **LimitaÃ§Ã£o**: NÃ£o penaliza similaridade entre pares negativos
- **Resultado**: Funciona, mas similaridade com classes erradas ainda Ã© alta

#### 2. Contrastive Loss (CLIP)
- **Objetivo**: Maximizar similaridade de pares positivos E minimizar similaridade de pares negativos
- **ImplementaÃ§Ã£o**: Cross-entropy simÃ©trica sobre matriz de similaridades
- **Vantagem**: Melhor separaÃ§Ã£o entre classes, similaridades mais discriminativas


## Resultados Esperados

### MSE Loss
- âœ… Recupera imagens corretas
- âŒ Alta similaridade com pares negativos (~0.7-0.9)
- NÃ£o discrimina bem entre classes

### Contrastive Loss (CLIP)
- âœ… Recupera imagens corretas
- âœ… Baixa similaridade com pares negativos (~0.2-0.4)
- âœ… Melhor separaÃ§Ã£o entre classes
- âœ… Busca mais robusta e discriminativa


## ğŸ“– ReferÃªncia

**Radford, Alec, et al. (2021)**  
[*"Learning Transferable Visual Models From Natural Language Supervision"*](https://arxiv.org/abs/2103.00020)  
