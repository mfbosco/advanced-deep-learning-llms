# 01 - Sentiment Analysis (AnÃ¡lise de Sentimentos)

Projeto de anÃ¡lise de sentimentos no dataset IMDB utilizando a abordagem Bag of Words (BoW) e redes neurais com PyTorch.

## ğŸ“‹ DescriÃ§Ã£o

Este projeto implementa um classificador de sentimentos binÃ¡rio (positivo/negativo) para reviews de filmes do dataset IMDB. A implementaÃ§Ã£o utiliza uma abordagem clÃ¡ssica de Bag of Words combinada com uma rede neural MLP (Multi-Layer Perceptron).

## ğŸ¯ Objetivos

- Implementar pipeline completo de processamento de texto
- Construir representaÃ§Ã£o Bag of Words eficiente
- Treinar modelo de classificaÃ§Ã£o binÃ¡ria
- Avaliar desempenho em dados de teste
- Otimizar processamento para uso em GPU


## ğŸ“Š Dataset

- **Fonte**: Stanford IMDB Dataset (via Hugging Face)
- **Tamanho**: 25,000 reviews para treino + 25,000 para teste
- **Classes**: BinÃ¡ria (0 = negativo, 1 = positivo)
- **Formato**: Texto livre (reviews em inglÃªs)

## ğŸ› ï¸ ImplementaÃ§Ã£o

### 1. PrÃ©-processamento e TokenizaÃ§Ã£o

```python
def pre_process(text):
    return re.sub(r'[^\w\s]', '', text).lower().split()
```

**CaracterÃ­sticas:**
- RemoÃ§Ã£o de pontuaÃ§Ãµes
- ConversÃ£o para minÃºsculas
- TokenizaÃ§Ã£o por espaÃ§os
- VocabulÃ¡rio limitado Ã s 20,000 palavras mais frequentes

### 2. RepresentaÃ§Ã£o Bag of Words

```python
class IMDBDataset(Dataset):
    def __init__(self, split, vocab):
        self.labels = torch.tensor(imdb_dic[split]['label'])
        texts = imdb_dic[split]['text']
        self.X = torch.zeros((len(texts), len(vocab)+1), dtype=torch.float32)
        for i, line in enumerate(texts):
            for word in tokenizer(line, vocab):
                self.X[i, word] = 1
```

**OtimizaÃ§Ãµes:**
- âœ… VetorizaÃ§Ã£o prÃ©-computada durante inicializaÃ§Ã£o
- âœ… Uso de tensores PyTorch nativos
- âœ… Evita reprocessamento a cada batch
- âœ… Suporte eficiente para GPU

### 3. Arquitetura do Modelo

```python
class OneHotMLP(nn.Module):
    def __init__(self, vocab_size):
        super(OneHotMLP, self).__init__()
        self.fc = nn.Linear(vocab_size + 1, 2)
```

**EspecificaÃ§Ãµes:**
- Modelo: MLP simples (Linear + Softmax)
- Input: Vetor BoW de tamanho vocab_size + 1
- Output: 2 classes (negativo/positivo)
- FunÃ§Ã£o de perda: CrossEntropyLoss
- Otimizador: SGD com learning rate 0.1

### 4. Treinamento

**ConfiguraÃ§Ã£o:**
- Split: 80% treino / 20% validaÃ§Ã£o
- Batch size: 32
- Ã‰pocas: 10
- Device: GPU (quando disponÃ­vel)

**Processo:**
- Loop de treino com backpropagation
- ValidaÃ§Ã£o a cada Ã©poca
- Monitoramento de loss e acurÃ¡cia

## ğŸ“ˆ Resultados

| MÃ©trica | Valor |
|---------|-------|
| Test Accuracy | **86.77%** |
| Treino | ~2s por Ã©poca (GPU) |
| Velocidade | ~10x mais rÃ¡pido com GPU |


## ğŸ” AnÃ¡lise de Performance

### Antes das OtimizaÃ§Ãµes
- â±ï¸ ~50-60s por Ã©poca (CPU)
- ğŸŒ Reprocessamento em cada batch
- ğŸ“‰ VocabulÃ¡rio inconsistente

### Depois das OtimizaÃ§Ãµes
- âš¡ ~2s por Ã©poca (GPU)
- ğŸš€ VetorizaÃ§Ã£o prÃ©-computada
- ğŸ“ˆ Pipeline unificado

## ğŸ“š Conceitos Abordados

- **Bag of Words**: RepresentaÃ§Ã£o vetorial de texto
- **TokenizaÃ§Ã£o**: Processamento e normalizaÃ§Ã£o de texto
- **VocabulÃ¡rio**: ConstruÃ§Ã£o e limitaÃ§Ã£o de features
- **MLP**: Redes neurais feedforward
- **Binary Classification**: ClassificaÃ§Ã£o binÃ¡ria
- **PyTorch Dataset**: ImplementaÃ§Ã£o eficiente de datasets
- **GPU Optimization**: Uso de CUDA para aceleraÃ§Ã£o

## ğŸ“ Aprendizados

1. ImportÃ¢ncia do prÃ©-processamento consistente
2. Impacto da otimizaÃ§Ã£o no tempo de treinamento
3. Trade-off entre tamanho de vocabulÃ¡rio e performance
4. BenefÃ­cios da vetorizaÃ§Ã£o prÃ©-computada
5. Uso eficiente de GPU em PyTorch

## ğŸ“ Notas

- Este projeto foi desenvolvido como parte do processo seletivo para o curso
- Foco em implementaÃ§Ã£o eficiente e otimizada
- Abordagem educacional com comentÃ¡rios explicativos
- VersÃ£o de referÃªncia: 13 de julho de 2025

## ğŸ”— ReferÃªncias

- [IMDB Dataset](https://huggingface.co/datasets/stanfordnlp/imdb)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

---

**Nota**: Para detalhes completos da implementaÃ§Ã£o, consulte o notebook com todas as cÃ©lulas e outputs.
