# 04 - BERT (Bidirectional Encoder Representations from Transformers)

Uso de **BERT prÃ©-treinado** como extrator de features para modelo de linguagem, combinando embeddings contextuais com MLP para prediÃ§Ã£o de prÃ³xima palavra.

## ğŸ¯ Objetivo

Implementar modelo de linguagem usando:
- BERT prÃ©-treinado (feature extractor)
- MLP para prediÃ§Ã£o de prÃ³xima palavra
- Dataset Machado de Assis (portuguÃªs)
- Loop de treinamento customizado

## ğŸ—‚ï¸ Estrutura

```
04-BERT/
â”œâ”€â”€ README.md
â”œâ”€â”€ exercicio_BERT.ipynb
â””â”€â”€ comentario-critico-BERT.pdf
```

## ğŸ“š Conceitos

**BERT** (Devlin et al., 2019):
- Modelo bidirecional (contexto esquerda + direita)
- PrÃ©-treinado com Masked Language Modeling (MLM)
- TransferÃªncia de aprendizado via embeddings contextuais

**Arquitetura:**
```
Input tokens (context_size)
    â†“
BertTokenizer (subword tokenization)
    â†“
BertModel.from_pretrained() [FROZEN/FINE-TUNED]
    â†“
Last hidden state â†’ Ãºltimo token embedding
    â†“
MLP compacto (D â†’ R â†’ vocab_size)
    â†“
CrossEntropyLoss
```

## ğŸ“Š Desafios e SoluÃ§Ãµes

| Desafio | SoluÃ§Ã£o |
|---------|---------|
| **Vocab muito grande** (30K tokens) | MLP com bottleneck: Dâ†’16â†’vocab_size |
| **Custo computacional** | Usar BERT-tiny (2 camadas, 128 dim) |
| **Contexto limitado** | Experimentar context_size = 5, 10, 20 |
| **Overfitting** | Congelar BERT, usar dropout |


## ğŸ” ComparaÃ§Ã£o

| Abordagem | Embeddings | Contexto | Performance |
|-----------|-----------|----------|-------------|
| **Bengio 2003** | EstÃ¡ticos | Fixo | Baseline |
| **Attention** | AprendÃ­veis | VariÃ¡vel | Melhor |
| **BERT** | **Contextuais** | **Bidirecional** | **Estado da arte** |

## ğŸ“– ReferÃªncia

**Devlin, J., et al. (2019)**  
[*"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"*](https://arxiv.org/abs/1810.04805)  


**InovaÃ§Ãµes:**
- Bidirecionalidade (vs GPT unidirecional)
- Masked Language Modeling (MLM)
- Next Sentence Prediction (NSP)
- Transfer learning para NLP


